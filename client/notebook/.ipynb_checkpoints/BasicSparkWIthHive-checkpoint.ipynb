{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5fded3f-a062-41f4-b2fe-7eb0889117dc",
   "metadata": {},
   "source": [
    "# PySpark의 Hive 연결 예제\n",
    "\n",
    "### 1. findspark를 통해 pyspark 등 라이브러리 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95fa308f-e2b9-49e7-b81a-f3bfe1f416e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/lib/spark-3.3.2-bin-hadoop3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ccde6-aaa1-46b6-911a-3caa78fb7bf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. SparkConf를 통해 configuration 추가하고, SparkContext 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36cdfb0-fa16-4ca7-a4e7-d4c207f78c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sconf = SparkConf()\n",
    "sconf.setAppName(\"Jupyter_Notebook\") \\\n",
    "        .setSparkHome(\"/usr/local/lib/spark-3.3.2-bin-hadoop3\") \\\n",
    "        .setMaster(\"yarn\") \\\n",
    "        .setExecutorEnv(\"client\") \\\n",
    "sc = SparkContext(conf=sconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cdbd14-2f79-4aa5-bc9e-02e9593143c3",
   "metadata": {},
   "source": [
    "### 3. Hadoop을 통해 Hive 설정을 받아오고 HiveContext를 자동으로 생성하는 enableHiveSupprot() 메소드 호출\n",
    "이를 위해선 HADOOP_CONF_DIR 변수가 설정 되어있어야 하고, 필요한 경우 Hive warehouse 경로인 spark.sql.warehouse.dir, Hive metastore 주소인 hive.metastore.uris 설정이 되어있어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56c57881-be3a-4f42-8e05-05711f1ba675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "hc = SparkSession.builder.appName(\"PySpark_Hive_Session\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad83318-c2b5-4635-bad7-eb54dcd767c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. HQL문을 전달하여 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4a39122-8838-4b04-8f33-b43ebe564f81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|deal_date|count|\n",
      "+---------+-----+\n",
      "| 20130108|  173|\n",
      "| 20130109|  173|\n",
      "| 20130110|  215|\n",
      "| 20130112|  206|\n",
      "| 20130111|  196|\n",
      "| 20130107|  167|\n",
      "| 20130105|  200|\n",
      "| 20130104|  100|\n",
      "| 20130113|   56|\n",
      "| 20130101|   27|\n",
      "| 20130106|   32|\n",
      "| 20130103|  126|\n",
      "| 20130102|  107|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trade_df = hc.sql(\"SELECT * FROM real_estate.trade;\")\n",
    "\n",
    "count_grouped_date_df = trade_df.groupBy(\"deal_date\").count()\n",
    "count_grouped_date_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfd1e7-fb5e-44d5-a1d3-0a41e2a99a00",
   "metadata": {},
   "source": [
    "### 5. Hive 세션과 SparkContext 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a5a9e6-002c-493a-b0d9-e98a6a4df469",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs.stop()\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
