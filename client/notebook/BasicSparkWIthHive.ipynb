{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95fa308f-e2b9-49e7-b81a-f3bfe1f416e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/lib/spark-3.3.2-bin-hadoop3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36cdfb0-fa16-4ca7-a4e7-d4c207f78c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/03 07:06:27 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:750)\n",
      "23/03/03 07:06:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sconf = SparkConf()\n",
    "sconf.setAppName(\"Jupyter_Notebook\").setSparkHome(\"/usr/local/lib/spark-3.3.2-bin-hadoop3\").setMaster(\"yarn\").setExecutorEnv(\"client\")\n",
    "sc = SparkContext(conf=sconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f49a1b-d392-4dcd-b023-09489bc227b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43msc\u001B[49m\u001B[38;5;241m.\u001B[39mstop()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56c57881-be3a-4f42-8e05-05711f1ba675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "hc = SparkSession.builder.appName(\"PySpark_Hive_Session\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4a39122-8838-4b04-8f33-b43ebe564f81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|deal_date|count|\n",
      "+---------+-----+\n",
      "| 20130108|  173|\n",
      "| 20130109|  173|\n",
      "| 20130110|  215|\n",
      "| 20130112|  206|\n",
      "| 20130111|  196|\n",
      "| 20130107|  167|\n",
      "| 20130105|  200|\n",
      "| 20130104|  100|\n",
      "| 20130113|   56|\n",
      "| 20130101|   27|\n",
      "| 20130106|   32|\n",
      "| 20130103|  126|\n",
      "| 20130102|  107|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trade_df = hc.sql(\"SELECT * FROM real_estate.trade;\")\n",
    "\n",
    "count_grouped_date_df = trade_df.groupBy(\"deal_date\").count()\n",
    "count_grouped_date_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a5a9e6-002c-493a-b0d9-e98a6a4df469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
